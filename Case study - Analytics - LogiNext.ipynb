{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790ae4f3",
   "metadata": {},
   "source": [
    "Please have a look into the code and follow the below texts for understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62e4a6a4",
   "metadata": {},
   "source": [
    "# Process for Location Analytics include:\n",
    "\n",
    "    1. Address Standardization\n",
    "    \n",
    "    2. Address Normalization\n",
    "    \n",
    "    3. Address Validation\n",
    "    \n",
    "    4. Address Verification\n",
    "    \n",
    "    5. Geocoding\n",
    "    \n",
    "    6. Feeding into Machine Learning Model with Active Learning (with New addresses also being added continuously into the databases and old addressess being verified and validated conitnuously)\n",
    "    \n",
    "    7. Reiterating the whole process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa39ca6",
   "metadata": {},
   "source": [
    "# Address Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ca93c",
   "metadata": {},
   "source": [
    "The relevant details (i.e. street number, apartment number, street name, city, state, and postal code) are in the correct formats and they pin to a specific and unique Location (i.e. Latititude and Longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01239e63",
   "metadata": {},
   "source": [
    "## Some of the reasons for Address discrepancies can be:\n",
    "\n",
    "1. Incomplete information - Missing street names, block numbers, or zip codes\n",
    "\n",
    "2. Invalid information - Fake addresses and zip codes\n",
    "\n",
    "3. Incorrect information - Typos, misspellings, formatting of abbreviations (i.e. CAL instead of CA for California)\n",
    "\n",
    "4. Inaccurate information - Wrong addresses or house numbers\n",
    "\n",
    "Also due to population growth, urbanization, and new construction many new addressess are always getting add up. Example in the United States, the Postal Service (USPS) adds 4,221 addresses to its delivery network every day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5afe9d1f",
   "metadata": {},
   "source": [
    "### Data Preparation Steps:\n",
    "\n",
    "    1. Normalization\n",
    "    2. Stemming\n",
    "    3. Lemmatization\n",
    "    4. Segmentation (tokenization)\n",
    "    5. Text rebuild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0343d74",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization consists on transforming the text to a canonical form to make them easy to be compared. \n",
    "\n",
    "Steps for Normalization:\n",
    "\n",
    "    1. Standardize encoding\n",
    "    2. Remove punctuation\n",
    "    3. Transform to lowercase\n",
    "    4. Remove stopwords and punctuation\n",
    "    5. Separate prefixes and suffixes that doesn’t contain information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4f3c5",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing words in different forms (conjugated verbs, plural) to a radical form. This step is not useful for addresses because most of the addresses are not in different forms. Proper names, for example, are very common in addresses and don’t benefit a lot from stemming.\n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the flexionated forms of the words so they can be analysed together.\n",
    "\n",
    "## Segmentation\n",
    "\n",
    "Segmentation is the task of breaking up the text into tokens, so each token can be analysed separately. \n",
    "For our case the address field can be broke down into preffixes, location, complements and suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861adb2",
   "metadata": {},
   "source": [
    "This is helpful because now we can match each part of the address with an existing canonical form without a lot of noise. Each of the fields can be further processed to extract more information, like the pincode number.\n",
    "\n",
    "## Parsing\n",
    "\n",
    "The next step is to parse the address. Parsing consists in break up the address string into fields that compose the address, breaking up of address into specific fields. To parse we have to assume a structure for the address.\n",
    "\n",
    "## Rebuilding the Text\n",
    "This task consists in rebuild the normalized text to a final form. It will be done after the match phase.\n",
    "\n",
    "## Identification and Match\n",
    "After cleaning up and normalizing the text we need to check if the value of the address exists in our in-house database. \n",
    "\n",
    "## Two approaches:\n",
    "\n",
    "### 1. Match with existing database\n",
    "Name Entity Recognition on address\n",
    "Match with  database\n",
    "If we have a database with the data considered correct, our job is to match the target addresses with the ones on this in the database. This is a match problem. We can attack this problem following these steps:\n",
    "\n",
    "### 2. Split address by field (prefix, location, suffixes)\n",
    "retrieve match candidates (search engine)\n",
    "Match address with candidates by similarity\n",
    "For this approach we’re going to work directly on the text patterns, without any kind of machine learning. The canonical database is usually provided by the Post Office.\n",
    "\n",
    "The match between two addresses is a way to check if two addresses are the same. For example, let’s say that we have in our canonical database the entry\n",
    "\n",
    "    Pincode     Location\t    City    State\n",
    "    400093\t    Mahakali Caves\tMumbai\tMH\n",
    "    NaN\t        Mahakali Road\tMumbai\tMH\n",
    "\n",
    "How to figure out which one is the best match? \n",
    "We could try to do an exact match: only the location strings that are exactly same are the same address. \n",
    "But this would miss lots of entries that could have typing errors but are otherwise valid addresses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba8d93",
   "metadata": {},
   "source": [
    "\n",
    "## Match\n",
    "One approach is to retrieve candidates from the in-house database that are similar to the address we want to normalize. Search engines do that using different strategies. We’re not going to detail this process, so let’s just say that our search engine returned candidates to be compared.\n",
    "\n",
    "For each of these candidates we do a comparison with our target address using some metric of similarity. There are several of such metrics:\n",
    "\n",
    "    1. Jaro distance\n",
    "    2. Jaro-Winkler distance\n",
    "    3. Cosine distance\n",
    "\n",
    "I've chosen Jaro-Winkler distance. We compare the target address with each of the candidates and rank by the similarity between them.\n",
    "\n",
    "## Search engines\n",
    "\n",
    "Search engines usually already make the string similarity comparison to retrive the candidates, so it could, in principle, already compute the similarity score withou the need to program it by ourselves. But sometimes the search engine similarity algorithm cannot be tuned to the type of text, like addresses. We also have more information than only the Location string, like the postal code and suffixes. This could help in the decision process.\n",
    "\n",
    "## NER\n",
    "Instead of using regular expressions to break up the address text into components we could create a Named Entity Recognizer and let it separate the address by fields.\n",
    "\n",
    "tag canonical database with relevant tags\n",
    "train CRF with tagged database\n",
    "classify each address\n",
    "match classified entity with canonical base\n",
    "\n",
    "## Decision process\n",
    "After the text normalization and match we hopefully have a list of candidates with a similarity score between the target and a canonical address. How we decide if the address is indeed the correct address? We can set a score threshold, for example, based on our experience, and test the error rate. We also can create a classification model and train manually with some entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf8668",
   "metadata": {},
   "source": [
    "## Address Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2413e4",
   "metadata": {},
   "source": [
    "Please have a look into the Address Normalization Folder for the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42727189",
   "metadata": {},
   "source": [
    "## Address Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584343f",
   "metadata": {},
   "source": [
    "## Address Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95aa3fe",
   "metadata": {},
   "source": [
    "# Geocoding using Geopy\n",
    "\n",
    "we can also building something similar to this in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268bfa4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f715c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa49be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messy_address = pd.read_excel(\"messy address.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba410ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw Address</th>\n",
       "      <th>Issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800 S Figueroa St, Los Angeles, CA</td>\n",
       "      <td>missing zipcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>515 W 7th St 2nd floor, Los Angeles, CA 90014</td>\n",
       "      <td>missing state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2701 S Vermont Ave, Los Angoles, CA 90007</td>\n",
       "      <td>misspelled city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7511 Raymonds Ave, Los Angeles, CA 90044</td>\n",
       "      <td>misspelled street name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7900 S Westen Ave, Los Angeles, 90047</td>\n",
       "      <td>misspelled street name, missing state</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Raw Address  \\\n",
       "0              800 S Figueroa St, Los Angeles, CA   \n",
       "1  515 W 7th St 2nd floor, Los Angeles, CA 90014    \n",
       "2       2701 S Vermont Ave, Los Angoles, CA 90007   \n",
       "3        7511 Raymonds Ave, Los Angeles, CA 90044   \n",
       "4           7900 S Westen Ave, Los Angeles, 90047   \n",
       "\n",
       "                                   Issue  \n",
       "0                        missing zipcode  \n",
       "1                          missing state  \n",
       "2                        misspelled city  \n",
       "3                 misspelled street name  \n",
       "4  misspelled street name, missing state  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messy_address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680e1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenStreetMap Free API \n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"Verified email address from OpenStreetMap\")\n",
    "# Google Maps Paid API\n",
    "# from geopy.geocoders import GoogleV3\n",
    "# geolocator = GoogleV3(api_key='Your Google Maps API Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0da724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_address(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address)\n",
    "        return location.address\n",
    "    except:\n",
    "        return ''\n",
    "messy_address['clean address'] = messy_address.apply(lambda x: extract_clean_address(x['Raw Address']) , axis =1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lat_long(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address)\n",
    "        return [location.latitude, location.longitude]\n",
    "    except:\n",
    "        return ''\n",
    "messy_address['lat_long'] = messy_address.apply(lambda x: extract_lat_long(x['Raw Address']) , axis =1)\n",
    "messy_address['latitude'] = messy_address.apply(lambda x: x['lat_long'][0] if x['lat_long'] != '' else '', axis =1)\n",
    "messy_address['longitude'] = messy_address.apply(lambda x: x['lat_long'][1] if x['lat_long'] != '' else '', axis =1)\n",
    "messy_address.drop(columns = ['lat_long'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e293d",
   "metadata": {},
   "source": [
    "## Feeding the updated results back to the ML model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
